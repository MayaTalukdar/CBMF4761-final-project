{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio import Phylo\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import io "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given an array of form:  \n",
    "$[[parent_k,child_k,branchlength_k\\ for\\ k\\ pairs\\ in\\ tree_i]\\ for\\ i\\ trees]$  \n",
    "1. Transform branch lengths to proportions of tree depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_depth(tree):\n",
    "    '''given a tree in newick format, return overall depth'''\n",
    "    \n",
    "    tree = Phylo.read(io.StringIO(tree), 'newick')\n",
    "    max_depth = max(tree.depths().values())\n",
    "    \n",
    "    return max_depth\n",
    "\n",
    "def branch_ratio(data, tree_data, index):\n",
    "    '''given a data_row with form [[parent ,child, branch_length]],\n",
    "    return data_row with ratios of branch lengths'''\n",
    "    \n",
    "    tree = tree_data[tree_data['dreamID'] == index]['ground'].item()\n",
    "    max_depth = tree_depth(tree)\n",
    "    \n",
    "    transformed_data = data #initialize\n",
    "    for i in range(len(data)):\n",
    "        transformed_data[i][2] = data[i][2] / max_depth\n",
    "    \n",
    "    return transformed_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert Parent,Child pairs to 20x1 mutation array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trit_det(parent, child):\n",
    "    '''Given two trits from parent node 1 and child node 2 joined by an edge,\n",
    "    return list alpha,beta determining mutation'''\n",
    "    \n",
    "    if parent == '1':\n",
    "        if child == '2':#1->2\n",
    "            alpha,beta = 0,1\n",
    "        else:#1->0\n",
    "            alpha,beta = 1,0\n",
    "    else:#no mutation\n",
    "        alpha,beta = 0,0\n",
    "    \n",
    "    return [alpha,beta]\n",
    "\n",
    "def barcode_det(parent, child):\n",
    "    '''Given two barcodes from parent node 1 and child node 2 joined by an edge,\n",
    "    return 10x2 array with rows alpha_i beta_i'''\n",
    "    \n",
    "    parent = str(parent)\n",
    "    child = str(child)\n",
    "    alpha_beta_array = np.zeros((10,2))\n",
    "    \n",
    "    for i in range(10):\n",
    "        alpha_beta_array[i, :] = trit_det(parent[i], child[i])\n",
    "    \n",
    "    return alpha_beta_array\n",
    "\n",
    "def convert_pair(tree):\n",
    "    '''Given a data row of form [[parent,child,branch_length]]\n",
    "    return row of form [1x20 mutations, branch_length]'''\n",
    "    \n",
    "    converted_tree = []#initialize\n",
    "    for pair in tree:\n",
    "        mut_array = barcode_det(pair[0], pair[1]).reshape((20, ))\n",
    "        converted_pair = [mut_array, pair[2]]\n",
    "        converted_tree.append(converted_pair)\n",
    "    \n",
    "    return converted_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Reformat data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: `item` has been deprecated and will be removed in a future version\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "def load_from_txt(data):\n",
    "    '''Given a .txt file of the data, return an array'''\n",
    "    loaded_data = []\n",
    "    \n",
    "    with open(data) as infile:\n",
    "        lines = infile.readlines()\n",
    "        for line in lines:\n",
    "            line_list = line[2:len(line)-4].split('], ')\n",
    "            new_line_list = []\n",
    "            for pair in line_list:#[parent,child,branchlength] objects\n",
    "                pair = pair[1:].split(', ')\n",
    "                new_pair = pair\n",
    "                new_pair[2] = float(pair[2])\n",
    "                new_line_list.append(new_pair)\n",
    "            loaded_data.append(new_line_list)\n",
    "            \n",
    "    return np.array(loaded_data)\n",
    "            \n",
    "\n",
    "DREAM_data = pd.read_csv('Data/DREAM_data_intMEMOIR.csv', sep = '\\t')\n",
    "DREAM_train = DREAM_data[30:]\n",
    "DREAM_test = DREAM_data[:30]\n",
    "\n",
    "load_from_txt('Data/testingDataFinalOutput.txt')\n",
    "test_data_raw = load_from_txt('Data/testingDataFinalOutput.txt')\n",
    "train_data_raw = load_from_txt('Data/trainingDataFinalOutput.txt')\n",
    "\n",
    "reformat_train = []\n",
    "reformat_test = []\n",
    "\n",
    "reformat_pair = [reformat_train, reformat_test]\n",
    "DREAM_pair = [DREAM_train, DREAM_test]\n",
    "data_pair = [train_data_raw, test_data_raw]\n",
    "\n",
    "\n",
    "for k in range(2): #0 = train, 1 = test\n",
    "    for i in range(1, data_pair[k].shape[0]):\n",
    "        tree_row = data_pair[k][i]\n",
    "        tree_row = branch_ratio(tree_row, DREAM_pair[k], i)\n",
    "        reformat_row = convert_pair(tree_row)\n",
    "        reformat_pair[k].append(np.array(reformat_row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Setup data for the NN:  \n",
    "    a. Train: will be split into minibatches by tree (list of DataLoader objects)  \n",
    "    b. Test: Will be a Nx2 array with columns (mutation array, branchlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_list = []\n",
    "\n",
    "for i in range(len(reformat_train)):\n",
    "    tree_i = np.array(reformat_train[i])\n",
    "    tree_i.shape = (len(reformat_train[i]), 2) # num_pairs x 2 array, columns are 1x20 mutation set and branch length\n",
    "    \n",
    "    train_input, train_label = np.vstack(tree_i[:, 0]), np.array(tree_i[:, 1], dtype = np.float32)\n",
    "    input_train_torch = torch.from_numpy(train_input).type(torch.FloatTensor).view(-1, 1, 20)\n",
    "    label_train_torch = torch.from_numpy(train_label).type(torch.FloatTensor)\n",
    "    \n",
    "    train_data = TensorDataset(input_train_torch, label_train_torch)\n",
    "    loader_list.append(DataLoader(train_data, batch_size = 1))#within tree minibatch, steps of stride 1\n",
    "    \n",
    "\n",
    "test_input = []\n",
    "test_label = []\n",
    "#extract all pair_data into train:test columns, no minibatching\n",
    "for i in range(len(reformat_test)):\n",
    "    for pair_data in reformat_test[i]:\n",
    "        test_input.append(pair_data[0])\n",
    "        test_label.append(pair_data[1])\n",
    "\n",
    "input_test_torch = torch.from_numpy(np.array(test_input)).type(torch.FloatTensor).view(-1, 1, 20)\n",
    "label_test_torch = torch.from_numpy(np.array(test_label)).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(\n",
      "  (fc): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class model(nn.Module):\n",
    "    '''1 layer (FC) Neural Network'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Define model module'''\n",
    "        super(model, self).__init__()\n",
    "        self.fc = nn.Linear(20, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Define model activation'''\n",
    "        return F.sigmoid(self.fc(x))\n",
    "\n",
    "BRANCH_MODEL = model()\n",
    "print(BRANCH_MODEL)\n",
    "#define the optimizer\n",
    "optimizer = optim.Adam(BRANCH_MODEL.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | train_loss: 0.017605541639114153\n",
      "Epoch: 5 | train_loss: 0.017592108202498825\n",
      "Epoch: 10 | train_loss: 0.01759073626788835\n",
      "Epoch: 15 | train_loss: 0.01758583706814409\n",
      "Epoch: 20 | train_loss: 0.017581437326569846\n",
      "Epoch: 25 | train_loss: 0.017577528108863365\n",
      "Epoch: 30 | train_loss: 0.017574051531113088\n",
      "Epoch: 35 | train_loss: 0.017570956166220533\n",
      "Epoch: 40 | train_loss: 0.017568202553387216\n",
      "Epoch: 45 | train_loss: 0.017565743765397983\n",
      "Epoch: 50 | train_loss: 0.017563549703090767\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "train_epoch_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS+1):\n",
    "    train_loss = []\n",
    "    \n",
    "    #here train_set is the dataset of trees\n",
    "    #each index correlates with set of [alpha_beta_array,branchlength]\n",
    "    #at this point branch length needs to be normalized\n",
    "    #for given tree \n",
    "    for train_loader in loader_list: #this would correlate with a DataLoader object\n",
    "\n",
    "        for batch_index, (train_data, train_label) in enumerate(train_loader):\n",
    "            \n",
    "            BRANCH_MODEL.train()\n",
    "            train_label_predicted = BRANCH_MODEL(train_data)\n",
    "            \n",
    "            #compute the loss\n",
    "            loss = F.smooth_l1_loss(train_label_predicted, train_label)\n",
    "            train_loss.append(loss.cpu().data.item())\n",
    "            \n",
    "            #reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "            #backpropagate the loss\n",
    "            loss.backward()\n",
    "            #update the parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_epoch_loss.append(np.mean(train_loss))\n",
    "        \n",
    "    if epoch%5 == 0:\n",
    "        print(\"Epoch: {} | train_loss: {}\".format(epoch, train_epoch_loss[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do: test network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
